import re
from langchain_aws import ChatBedrockConverse
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from typing import List, Sequence
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langgraph.graph import END, MessageGraph, START

import logging


class ReflectionGraph:

    logger = logging.getLogger(__name__)

    #default inspector/reviewer prompt. can be overriden.
    inspector_prompt = """ 
        You are a detailed oriented reviewer.

        Your task is to review the output generated by the previous AI system and assess whether it is correct or not in relation to the instructions it was given.

        skip the preamble. 

        if you agree with the previous AI system return True in <valid> tag and rewrite the answer in <answer> tag.

        if you disagree, return False in <valid> tag and explain why in <thinking> tag.
    """

    #default rework prompt. can be overriden.
    rework_prompt = """
        Take into consideration the previous feedbacks from the review and output your revised response in <answer> tag.
        DO NOT explain again your reasoning as you have already done it before.
    """

    #ChatBedrockConverse object with our llm configuration.
    actor_llm = None
    evaluator_llm = None

    #we use a generic langchain chain as container for our messages in the chatmessage graph.
    empty_prompt_template = None
    generic_chain = None

    #graph nodes names
    generate_node_name = "generate"
    reflect_node_name = "reflect"

    #compiled graph
    compiled_graph = None

    #simple function to extract the answer from xml tag
    @staticmethod
    def extract_answer(text, tag="answer"):
        pattern = f'<{tag}>(.*?)</{tag}>'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1)
        else:
            return None 

    #Methods to use as conditions for our edges.
    def should_continue_after_generate(self, messages: List[BaseMessage]):
        #stop generation after 3 iterations
        if len(messages) > 6:
            self.logger.debug("Proceed to END after 3 iterations")
            return END
        
        self.logger.debug("should_continue_after_generate: Proceed to reflect")
        return "reflect"

    def should_continue_after_reflect(self, messages: List[BaseMessage]):
        #stop if inspector is confirming the classification
        last_msg = messages[-1]
        is_valid = ReflectionGraph.extract_answer(last_msg.content, tag="valid")
        self.logger.debug(f"is_valid:{is_valid}")
        if is_valid != None and is_valid == "True":
            self.logger.debug("Proceed to END as is_valid is True")
            return END
        
        self.logger.debug("should_continue_after_reflect: Proceed generation")
        return "generate"
    
    async def generation_node(self,messages: Sequence[BaseMessage]):
        self.logger.debug("----------------------------- generation ----------------------------------\n")
        self.logger.debug(f"Messages:{messages}")

        #first iteration with original prompt and transcript
        if len(messages) == 1:
            #invoke chain with original prompt and transcript contained in HumanMessage
            res = await self.generic_chain_actor.ainvoke({"messages": messages})
            #we return an AIMessage to keep the alternance between Human/AI required by Anthropic models.
            return AIMessage(content=res.content)
        
        #subsequent iterations with rework prompt
        else:
            rework_prompt_template = ChatPromptTemplate.from_messages(
                [("human", self.rework_prompt)]
            )

            human_message = HumanMessage(
                    content=rework_prompt_template.format()
            )
            res = await self.generic_chain_actor.ainvoke({"messages": messages + [human_message]})
            return  [human_message, AIMessage(content=res.content)]

    async def reflection_node(self, messages: Sequence[BaseMessage]) -> List[BaseMessage]:
        self.logger.debug("----------------------------- reflection ----------------------------------\n")
        self.logger.debug(f"Messages:{messages}")

        human_message = HumanMessage(
            content=self.inspector_prompt
        )
        
        res = await self.generic_chain_evaluator.ainvoke({"messages": messages + [human_message]})
        return [human_message, AIMessage(content=res.content)]
    

    #init
    def __init__(self, actor_llm:ChatBedrockConverse, evaluator_llm:ChatBedrockConverse, inspector_prompt:str="", rework_prompt:str="", debug=False) -> None:

        # Set the logger level based on the debug parameter
        if debug:
            self.logger.setLevel(logging.DEBUG)
        else:
            self.logger.setLevel(logging.INFO)

        # Add a StreamHandler to the logger
        stream_handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        stream_handler.setFormatter(formatter)
        self.logger.addHandler(stream_handler)
        
        #initialising variables
        self.actor_llm = actor_llm
        self.evaluator_llm = evaluator_llm

        if inspector_prompt:
            self.inspector_prompt = inspector_prompt
        if rework_prompt:
            self.rework_prompt = rework_prompt

        #We use this as a container for the prompt we want to pass
        self.empty_prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "",
                ),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )
        #build the chain with that empty prompt template
        self.generic_chain_actor = self.empty_prompt_template | actor_llm
        self.generic_chain_evaluator = self.empty_prompt_template | evaluator_llm

        #build messagegraph
        builder = MessageGraph()

        #add nodes
        builder.add_node(self.generate_node_name, self.generation_node)
        builder.add_node(self.reflect_node_name, self.reflection_node)

        #add edges
        builder.add_edge(START, self.generate_node_name)
        builder.add_edge(self.generate_node_name, self.reflect_node_name)

        #add conditions between edges
        builder.add_conditional_edges(self.generate_node_name, self.should_continue_after_generate)
        builder.add_conditional_edges(self.reflect_node_name, self.should_continue_after_reflect)

        self.compiled_graph = builder.compile()

        self.logger.debug("ReflectionGraph instance created")

    #run the compiled graph by passing the main_prompt as the first instruction.
    async def run_graph(self, main_prompt):
        messages = []
        initial_msg = HumanMessage(
            content=main_prompt
        )
        messages.append(initial_msg)

        last_event = None

        async for event in self.compiled_graph.astream(messages):
            self.logger.debug(event)
            last_event = event
        
        self.logger.debug(f"last event:{last_event}")

        return ReflectionGraph.extract_answer(last_event["reflect"][-1].content)
        

    
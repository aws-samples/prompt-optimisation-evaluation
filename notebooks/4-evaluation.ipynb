{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r sentiment_categorisation_prompt_id\n",
    "%store -r summarisation_prompt_id\n",
    "%store -r extraction_prompt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or set them manually\n",
    "#sentiment_categorisation_prompt_id = \"\"\n",
    "#summarisation_prompt_id = \"\"\n",
    "#extraction_prompt_id = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.2.11\n",
    "!pip install -q  evaluate==0.4.2\n",
    "!pip install -q rapidfuzz==3.9.5 \n",
    "!pip install -q bert-score==0.3.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import importlib\n",
    "\n",
    "#adding our utils library to sys path\n",
    "import sys\n",
    "sys.path.append(\"../src/utils/\")\n",
    "import llm_utils\n",
    "\n",
    "importlib.reload(llm_utils)\n",
    "\n",
    "session = boto3.Session()\n",
    "region_name = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a ranges of different evaluator libraries to cover the need of our 3 use cases.\n",
    "\n",
    "Note that RAG related metrics (e.g. faithfulness, context precision, context recall) will be covered in the next notebook.\n",
    "\n",
    "For our Sentiment Classification, we'll use a straightforward string match to measure accuracy (correct answer/all answer) and string distance measure as well.\n",
    "\n",
    "For our summarisation task, we'll use the ROUGE metrics (ROUGE1 and ROUGEL) \n",
    "\n",
    "For our theme extraction task, we'll use various metrics including semantic similarity and Correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation libraries\n",
    "\n",
    "There are various libraries out there that you can use. We're just listing a few well known ones that are covering our needs for our scenarios' evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Evaluation\n",
    "\n",
    "Langchain includes 3 types of Evaluators:\n",
    "\n",
    "- String Evaluators: A string evaluator is a component within LangChain designed to assess the performance of a language model by comparing its generated outputs (predictions) to a reference string or an input. \n",
    "- Comparison Evaluators: Comparison evaluators in LangChain help measure two different chains or LLM outputs.\n",
    "- Trajectory Evaluators: Provide a more holistic approach to evaluating an agent. These evaluators assess the full sequence of actions taken by an agent and their corresponding responses, it is referred as the \"trajectory\".\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/\n",
    "\n",
    "For reference, see below the class storing Prompts used for Criteria evaluation: \n",
    "https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/evaluation/criteria/eval_chain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Evaluate\n",
    "\n",
    "The HF Evaluate library library covers various metrics used to evaluate ML models and dataset including traditional NLP metrics that are relevant for our scenarios' evaluation.\n",
    "\n",
    "https://github.com/huggingface/evaluate\n",
    "https://huggingface.co/docs/evaluate/en/base_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Amazon Bedrock Evaluation\n",
    "\n",
    "Amazon Bedrock supports model evaluation jobs. The results of a model evaluation job allow you to compare model outputs, and then choose the model best suited for your downstream generative AI applications.\n",
    "\n",
    "It does not support yet prompt evaluation metrics required to optimise your prompt for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our groundtruth datasets generated in notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = llm_utils.load_dict_from_json(\"../generated/groundtruth/sentiment_gt.json\")\n",
    "summary_data = llm_utils.load_dict_from_json(\"../generated/groundtruth/summary_gt.json\")\n",
    "extraction_data = llm_utils.load_dict_from_json(\"../generated/groundtruth/extraction_gt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will record our scores in that dict\n",
    "evaluation_scores = dict()\n",
    "evaluation_scores[\"sentiment\"] = []\n",
    "evaluation_scores[\"summarisation\"] = []\n",
    "evaluation_scores[\"extraction\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Simply calculated as correct answer / all answer, using a string match evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import ExactMatchStringEvaluator\n",
    "\n",
    "def generic_evaluate(data, evaluator):\n",
    "\n",
    "    #getting the size of our dataset\n",
    "    data_length = len(data[\"question\"])\n",
    "\n",
    "    #to store the scores that we'll average after.\n",
    "    scores = []\n",
    "\n",
    "    #calculating scores for each question/answer/groundtruth\n",
    "    for i in range(data_length):\n",
    "        score = evaluator.evaluate_strings(\n",
    "            prediction=data[\"answer\"][i],\n",
    "            reference=data[\"groundtruth\"][i],\n",
    "        )\n",
    "        scores.append(float(score[\"score\"]))\n",
    "\n",
    "    #calculate average across all scores\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    \n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exact match evaluator\n",
    "exact_match_evaluator = ExactMatchStringEvaluator()\n",
    "\n",
    "average_exact_match_score = generic_evaluate(sentiment_data, exact_match_evaluator)\n",
    "print(f\"Average accuracy score: {average_exact_match_score}\")\n",
    "\n",
    "#storing value aside\n",
    "evaluation_scores[\"sentiment\"].append({\"exact_match_accuracy\":average_exact_match_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer/Groundtruth distance\n",
    "\n",
    "The previous exact match accuracy metrics does not estimate the scale of the error if the string do not match. Assuming that the groundtruth is \"Very Negative\", a \"Negative\" response would in theory be close than \"Positive\" for example.\n",
    "\n",
    "There are different ways to measure the distance between answers and groundtruth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding distance / Semantic Similarity\n",
    "To measure semantic similarity (or dissimilarity) between a prediction and a reference label string, you could use a vector distance metric the two embedded representations using the embedding_distance evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EmbeddingDistance\n",
    "\n",
    "list(EmbeddingDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model_id = \"cohere.embed-english-v3\"\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(region_name=region_name,\n",
    "                                       model_id = embedding_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the below cell should take 1-2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# configure evaluator with our embeddings model and picking the cosine distance.\n",
    "embedding_distance_evaluator = load_evaluator(\"embedding_distance\", embeddings=bedrock_embeddings, distance_metric=EmbeddingDistance.COSINE)\n",
    "\n",
    "average_embeddings_distance_score = generic_evaluate(sentiment_data, embedding_distance_evaluator)\n",
    "print(f\"Embedding distance score: {average_embeddings_distance_score}\")\n",
    "\n",
    "#storing value aside\n",
    "evaluation_scores[\"sentiment\"].append({\"embeddings_distance\":average_embeddings_distance_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Distance\n",
    "\n",
    "The \"String distance\" can be measured using techniques like the Levenshtein distance (Wikipedia). It is a string metric for measuring the difference between two sequences. The Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "string_distance_evaluator = load_evaluator(\"string_distance\")\n",
    "\n",
    "average_string_distance_score = generic_evaluate(sentiment_data, string_distance_evaluator)\n",
    "print(f\"Average string distance score: {average_string_distance_score}\")\n",
    "\n",
    "#storing value aside\n",
    "evaluation_scores[\"sentiment\"].append({\"string_distance\":average_string_distance_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n",
    "\n",
    "The ROUGE-N (ROUGE-1, ROUGE-2) Score is calculated by taking the number of overlapping n-grams and dividing it by the total number of n-grams in the reference summary.\n",
    "\n",
    "ROUGE-L - The Longest Common Subsequence (LCS) method identifies the longest sequence of words that appear in the same order in both the reference and machine-generated summaries.\n",
    "\n",
    "ROUGE-LSum applies the ROUGE-L calculation method at the sentence level and then aggregates all the results for the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=summary_data[\"answer\"], references=summary_data[\"groundtruth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_scores[\"summarisation\"].append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity / BERTScore\n",
    "\n",
    "Semantic similarity is also a good option to compare two summarised paragraph and determine their distance using either cosine similarity or euclidean distance.\n",
    "\n",
    "We could use the langchain library and calculate the embedding distance but instead we're using BERTScore which works similarly but adds summarisation specific features.\n",
    "\n",
    "https://github.com/Tiiiger/bert_score\n",
    "\n",
    "for model_type, see:\n",
    "https://github.com/Tiiiger/bert_score/blob/19e7f551fe4fa43fdd07b8129ae947015b902b2d/bert_score/utils.py#L64\n",
    "\n",
    "Note that the below code will download the model you select in model_type. The selected model below is 3G and will take around 3min to download then 1min to do the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = extraction_data[\"answer\"]\n",
    "references = extraction_data[\"groundtruth\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\", nthreads=10, model_type=\"microsoft/deberta-xlarge-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating averages\n",
    "total_len = len(results[\"precision\"])\n",
    "bert_precision_avg = sum(results[\"precision\"])/total_len\n",
    "bert_recall_avg = sum(results[\"recall\"])/total_len\n",
    "bert_f1_avg = sum(results[\"f1\"])/total_len\n",
    "\n",
    "#adding the result to our combined result dict.\n",
    "evaluation_scores[\"summarisation\"].append({\"BERTScore-precision\":bert_precision_avg, \"BERTScore-recall\":bert_recall_avg, \"BERTScore-f1\":bert_f1_avg})\n",
    "\n",
    "print(f\"Precision={bert_precision_avg}\")\n",
    "print(f\"Recall={bert_recall_avg}\")\n",
    "print(f\"F1={bert_f1_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Evaluation\n",
    "\n",
    "We can use different approach to evaluate the alignment of our generated answers and the ground truth.\n",
    "Semantic Similarity would make sense as well as Correctness which determines whether an LLM output is factually correct based on some ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "We are using the langchain labeled_criteria evaluator with criteria=correctness. You will notice that the evaluator is using a llm to extract and compare facts between prediction, reference and input (instruction + transcript)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "\n",
    "#retrieve prompt details from Bedrock as it needs to be passed along with the transcript as an input.\n",
    "extraction_prompt = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=extraction_prompt_id\n",
    "    )\n",
    "#parse the response and retrieve the elements we need for later.\n",
    "extraction_prompt_dict = llm_utils.get_elts_from_prompt_get_response(extraction_prompt)\n",
    "\n",
    "eval_llm = ChatBedrockConverse(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    max_tokens = 4096,\n",
    "    temperature = 0,\n",
    "    top_p = 0.6\n",
    ")\n",
    "\n",
    "#evaluator\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=eval_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below should take 2-3min to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def run_evaluator(prompt_dict, question, answer, groundtruth, scores):\n",
    "    input_question_transcript = prompt_dict[\"prompt_text\"].format(transcript=question)\n",
    "\n",
    "    score = evaluator.evaluate_strings(\n",
    "        input=input_question_transcript,\n",
    "        prediction=answer,\n",
    "        reference=groundtruth,\n",
    "    )\n",
    "    scores.append(float(score[\"score\"]))\n",
    "\n",
    "\n",
    "#to store the scores that we'll average after.\n",
    "scores = []\n",
    "\n",
    "data_size = len(extraction_data[\"question\"])\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    \n",
    "    futures = []\n",
    "\n",
    "    for i in range(data_size):\n",
    "\n",
    "        futures.append(executor.submit(run_evaluator,\n",
    "                                        extraction_prompt_dict, \n",
    "                                        extraction_data[\"question\"][i], \n",
    "                                        extraction_data[\"answer\"][i], \n",
    "                                        extraction_data[\"groundtruth\"][i], scores))\n",
    "\n",
    "\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "\n",
    "    #calculate average across all scores\n",
    "    average_score = sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_scores[\"extraction\"].append({\"correctness\":average_score})\n",
    "\n",
    "print(f\"correctness average score:{average_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "Again here we use embedding distance to measure the semantic similarity between 2 lists of themes.\n",
    "\n",
    "The below cell should take 1-2min to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we reuse the same evaluator as previously\n",
    "\n",
    "average_embeddings_distance_score = generic_evaluate(extraction_data, embedding_distance_evaluator)\n",
    "print(f\"Embedding distance score: {average_embeddings_distance_score}\")\n",
    "\n",
    "#storing value aside\n",
    "evaluation_scores[\"extraction\"].append({\"embeddings_distance\":average_embeddings_distance_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Printing our computed metrics. \n",
    "\n",
    "Note that in a normal evaluation workflow you'd compare those values with previous scores to understand whether the new version of the prompt is not introducing any regression.\n",
    "\n",
    "For the sake of our example, we assume that it's not and that we're proceed with deploying our 3 prompts in production and store the scores as tags/metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formated_metrics(key):\n",
    "    res_dict = dict()\n",
    "    for elt in evaluation_scores[key]:\n",
    "        for key2 in elt.keys():\n",
    "            name = key + \"-\" + key2\n",
    "            value = elt[key2]\n",
    "            res_dict[name] = str(value)\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(get_formated_metrics(\"sentiment\"))\n",
    "pprint.pprint(get_formated_metrics(\"summarisation\"))\n",
    "pprint.pprint(get_formated_metrics(\"extraction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date\n",
    "today = datetime.now()\n",
    "formatted_date = today.strftime(\"%d-%m-%y\")\n",
    "\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "\n",
    "response_sentiment = bedrock_agent_client.create_prompt_version(\n",
    "    description=\"version created on:\" + formatted_date,\n",
    "    promptIdentifier=sentiment_categorisation_prompt_id,\n",
    "    tags=get_formated_metrics(\"sentiment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_summarisation = bedrock_agent_client.create_prompt_version(\n",
    "    description=\"version created on:\" + formatted_date,\n",
    "    promptIdentifier=summarisation_prompt_id,\n",
    "    tags=get_formated_metrics(\"summarisation\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_extraction = bedrock_agent_client.create_prompt_version(\n",
    "    description=\"version created on:\" + formatted_date,\n",
    "    promptIdentifier=extraction_prompt_id,\n",
    "    tags=get_formated_metrics(\"extraction\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

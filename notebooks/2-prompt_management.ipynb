{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3==1.34.149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import importlib\n",
    "\n",
    "#adding our utils library to sys path\n",
    "import sys\n",
    "sys.path.append(\"../src/utils/\")\n",
    "import llm_utils\n",
    "\n",
    "importlib.reload(llm_utils)\n",
    "\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the transcript for testing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = llm_utils.load_jsonlines_file(\"../generated/transcripts/transcripts.jsonl\")\n",
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Prompt Management\n",
    "At the time this workshop is created, Amazon Bedrock Prompt Management is a recently released feature still in preview. At the moment, it does not support system_prompt and would require the creation of 2 prompt templates. \n",
    "Considering this, we're storing the whole prompt including the system prompt within one single prompt template.\n",
    "\n",
    "Also note that we're only creating the prompt in this notebook. We will \"deploy\" the prompt by creating a version in notebook 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts that will be used for optimisation and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment categorisation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt_template = \"\"\"\n",
    "You are a professional detailed oriented sentiment analyser.\n",
    "\n",
    "Your task is to categorise the sentiment from the following conversation transcript between a customer and a call centre agent in <transcript> tag.\n",
    "\n",
    "<transcript>\n",
    "{transcript}\n",
    "</transcript>\n",
    "\n",
    "Think of the problem step by step:\n",
    "1. Read carefully through the transcript.\n",
    "2. Review the examples in <examples> tag to calibrate your response.\n",
    "3. Categorise the conversation's sentiment into one of the following categories:\n",
    "    - Very Positive\n",
    "    - Positive\n",
    "    - Neutral\n",
    "    - Negative\n",
    "    - Very Negative\n",
    "4. Skip the preamble and provide your response in <answer> tag.\n",
    "\n",
    "<examples>\n",
    "    <example>\n",
    "        <example_transcript>Hi, I'm calling to let you know that your service is exceptional, I love the variety and the overall UI. I would have just a recommendation to add a favourite functionality to your features.</example_transcript>\n",
    "        <answer>Very Positive<answer>\n",
    "    </example>\n",
    "    <example>\n",
    "        <example_transcript>Hi, I'm calling to inquire about any promotional offers or discounts you might have available.</example_transcript>\n",
    "        <answer>Positive<answer>\n",
    "    </example>\n",
    "    <example>\n",
    "        <example_transcript>Hi, I need to cancel my account. I'm moving to a different city, and your service isn't available there.</example_transcript>\n",
    "        <answer>Neutral<answer>\n",
    "    </example>\n",
    "    <example>\n",
    "        <example_transcript>I've been a loyal customer for years, and I'm getting really impatient with the lack of new content.</example_transcript>\n",
    "        <answer>Negative<answer>\n",
    "    </example>\n",
    "    <example>\n",
    "        <example_transcript>I'm so angry right now! The video quality on your streaming service is terrible. It keeps buffering and pixelating, making it impossible to watch anything.</example_transcript>\n",
    "        <answer>Very Negative<answer>\n",
    "    </example>\n",
    "<examples>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Amazon Bedrock prompt management to store the original  version of our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_categorisation_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "sentiment_prompt_name = \"sentiment_categorisation\"\n",
    "try:\n",
    "    sentiment_prompt_response = bedrock_agent_client.create_prompt(\n",
    "        defaultVariant='v1',\n",
    "        description='prompt used in the context of sentiment categorisation of a conversation between a user and a call centre agent',\n",
    "        name=sentiment_prompt_name,\n",
    "        tags={\n",
    "            'project-name': 'evaluation_workshop'\n",
    "        },\n",
    "        variants=[\n",
    "            {\n",
    "                'inferenceConfiguration': {\n",
    "                    'text': {\n",
    "                        'maxTokens': 400,\n",
    "                        'stopSequences': [\n",
    "                            '</answer>',\n",
    "                        ],\n",
    "                        'temperature': 0,\n",
    "                        'topP': 0.8\n",
    "                    }\n",
    "                },\n",
    "                'modelId': sentiment_categorisation_model_id,\n",
    "                'name': 'v1',\n",
    "                'templateConfiguration': {\n",
    "                    'text': {\n",
    "                        'inputVariables': [\n",
    "                            {\n",
    "                                'name': 'transcript'\n",
    "                            },\n",
    "                        ],\n",
    "                        'text': sentiment_prompt_template\n",
    "                    }\n",
    "                },\n",
    "                'templateType': 'TEXT'\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Prompt with similar name already exists, retrieving prompt instead\")\n",
    "\n",
    "    #get id for the already created prompt\n",
    "    id =\"\"\n",
    "    list_prompts = bedrock_agent_client.list_prompts(maxResults=10)\n",
    "    for prompt in list_prompts['promptSummaries']:\n",
    "        if prompt[\"name\"] == sentiment_prompt_name:\n",
    "            id = prompt[\"id\"]\n",
    "            break\n",
    "    \n",
    "    #retrieve prompt\n",
    "    sentiment_prompt_response = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=id\n",
    "    )\n",
    "\n",
    "sentiment_categorisation_prompt_id = sentiment_prompt_response['id']\n",
    "sentiment_categorisation_prompt_arn = sentiment_prompt_response['arn']\n",
    "\n",
    "print(f\"Sentiment prompt id: {sentiment_categorisation_prompt_id}\")\n",
    "print(f\"Sentiment prompt arn: {sentiment_categorisation_prompt_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarisation_prompt_template = \"\"\"\n",
    "You are a highly skilled and professional writer.\n",
    "\n",
    "Your task is to summarise the conversation between a customer and a call centre agent in <transcript> tag.\n",
    "\n",
    "<transcript>\n",
    "{transcript}\n",
    "</transcript>\n",
    "\n",
    "Read carefully through the transcript.\n",
    "\n",
    "Make sure to mention the important parts of the conversation including: reason for the call, problems encountered by the customer, tone of the conversation, resolution and next steps.\n",
    "\n",
    "Provide the summary in a <answer> tag.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Amazon Bedrock prompt management to store the original  version of our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarisation_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "summarisation_prompt_name = \"summarisation\"\n",
    "try:\n",
    "    summarisation_prompt_response = bedrock_agent_client.create_prompt(\n",
    "        defaultVariant='v1',\n",
    "        description='prompt used in the context of summarisation of a conversation between a user and a call centre agent',\n",
    "        name=summarisation_prompt_name,\n",
    "        tags={\n",
    "            'project-name': 'evaluation_workshop'\n",
    "        },\n",
    "        variants=[\n",
    "            {\n",
    "                'inferenceConfiguration': {\n",
    "                    'text': {\n",
    "                        'maxTokens': 2048,\n",
    "                        'stopSequences': [\n",
    "                            '</answer>',\n",
    "                        ],\n",
    "                        'temperature': 0,\n",
    "                        'topP': 0.8\n",
    "                    }\n",
    "                },\n",
    "                'modelId': summarisation_model_id,\n",
    "                'name': 'v1',\n",
    "                'templateConfiguration': {\n",
    "                    'text': {\n",
    "                        'inputVariables': [\n",
    "                            {\n",
    "                                'name': 'transcript'\n",
    "                            },\n",
    "                        ],\n",
    "                        'text': summarisation_prompt_template\n",
    "                    }\n",
    "                },\n",
    "                'templateType': 'TEXT'\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Prompt with similar name already exists, retrieving prompt instead\")\n",
    "\n",
    "    #get id for the already created prompt\n",
    "    id =\"\"\n",
    "    list_prompts = bedrock_agent_client.list_prompts(maxResults=10)\n",
    "    for prompt in list_prompts['promptSummaries']:\n",
    "        if prompt[\"name\"] == summarisation_prompt_name:\n",
    "            id = prompt[\"id\"]\n",
    "            break\n",
    "    \n",
    "    #retrieve prompt\n",
    "    summarisation_prompt_response = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=id\n",
    "    )\n",
    "\n",
    "summarisation_prompt_id = summarisation_prompt_response['id']\n",
    "summarisation_prompt_arn = summarisation_prompt_response['arn']\n",
    "\n",
    "print(f\"Summarisation prompt id: {summarisation_prompt_id}\")\n",
    "print(f\"Summarisation prompt arn: {summarisation_prompt_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theme extraction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompt_template = \"\"\"\n",
    "You are an intelligent information extraction assistant.\n",
    "\n",
    "Your task is to analyse the transcript of a conversation between a customer and a call centre agent in <transcript> tag and extract the key themes.\n",
    "\n",
    "<transcript>\n",
    "{transcript}\n",
    "</transcript>\n",
    "\n",
    "Read carefully through the transcript.\n",
    "\n",
    "Provide <number>3</number> key themes or topics that are important in the conversation in <answer> tag as shown in the example below.\n",
    "\n",
    "<example>\n",
    "    <answer>technical issue, lack of transparence of policy, business impact</answer>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Amazon Bedrock prompt management to store the original  version of our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "extraction_prompt_name = \"theme_extraction\"\n",
    "\n",
    "try:\n",
    "    extraction_prompt_response = bedrock_agent_client.create_prompt(\n",
    "        defaultVariant='v1',\n",
    "        description='prompt used in the context of theme extraction from a conversation between a user and a call centre agent',\n",
    "        name=extraction_prompt_name,\n",
    "        tags={\n",
    "            'project-name': 'evaluation_workshop'\n",
    "        },\n",
    "        variants=[\n",
    "            {\n",
    "                'inferenceConfiguration': {\n",
    "                    'text': {\n",
    "                        'maxTokens': 2048,\n",
    "                        'stopSequences': [\n",
    "                            '</answer>',\n",
    "                        ],\n",
    "                        'temperature': 0,\n",
    "                        'topP': 0.8\n",
    "                    }\n",
    "                },\n",
    "                'modelId': extraction_model_id,\n",
    "                'name': 'v1',\n",
    "                'templateConfiguration': {\n",
    "                    'text': {\n",
    "                        'inputVariables': [\n",
    "                            {\n",
    "                                'name': 'transcript'\n",
    "                            },\n",
    "                        ],\n",
    "                        'text': extraction_prompt_template\n",
    "                    }\n",
    "                },\n",
    "                'templateType': 'TEXT'\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Prompt with similar name already exists, retrieving prompt instead\")\n",
    "\n",
    "    #get id for the already created prompt\n",
    "    id =\"\"\n",
    "    list_prompts = bedrock_agent_client.list_prompts(maxResults=10)\n",
    "    for prompt in list_prompts['promptSummaries']:\n",
    "        if prompt[\"name\"] == extraction_prompt_name:\n",
    "            id = prompt[\"id\"]\n",
    "            break\n",
    "    \n",
    "    #retrieve prompt\n",
    "    extraction_prompt_response = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=id\n",
    "    )\n",
    "\n",
    "extraction_prompt_id = extraction_prompt_response['id']\n",
    "extraction_prompt_arn = extraction_prompt_response['arn']\n",
    "\n",
    "print(f\"Extraction prompt id: {extraction_prompt_id}\")\n",
    "print(f\"Extraction prompt arn: {extraction_prompt_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all created prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_client.list_prompts(\n",
    "    maxResults=10\n",
    ")\n",
    "response['promptSummaries']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store sentiment_categorisation_prompt_id\n",
    "%store sentiment_categorisation_prompt_arn\n",
    "%store summarisation_prompt_id\n",
    "%store summarisation_prompt_arn\n",
    "%store extraction_prompt_id\n",
    "%store extraction_prompt_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

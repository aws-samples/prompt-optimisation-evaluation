{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r sentiment_categorisation_prompt_id\n",
    "%store -r summarisation_prompt_id\n",
    "%store -r extraction_prompt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or set them manually\n",
    "#sentiment_categorisation_prompt_id = \"\"\n",
    "#summarisation_prompt_id = \"\"\n",
    "#extraction_prompt_id = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3==1.34.149\n",
    "!pip install -q langgraph==0.1.17\n",
    "!pip install -q langchain==0.2.11\n",
    "!pip install -q langchain-community==0.2.10\n",
    "!pip install -q langchain-aws==0.1.12\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import importlib\n",
    "\n",
    "#adding our utils library to sys path\n",
    "import sys\n",
    "sys.path.append(\"../src/utils/\")\n",
    "import llm_utils\n",
    "import reflection_graph\n",
    "from reflection_graph import ReflectionGraph\n",
    "\n",
    "importlib.reload(llm_utils)\n",
    "importlib.reload(reflection_graph)\n",
    "\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Transcripts to be used later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = llm_utils.load_jsonlines_file(\"../generated/transcripts/transcripts.jsonl\")\n",
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve prompt from Bedrock Prompt Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "\n",
    "#retrieve prompt details from Bedrock\n",
    "sentiment_categorisation_prompt = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=sentiment_categorisation_prompt_id\n",
    "    )\n",
    "#parse the response and retrieve the elements we need for later.\n",
    "sentiment_prompt_dict = llm_utils.get_elts_from_prompt_get_response(sentiment_categorisation_prompt)\n",
    "\n",
    "#retrieve prompt details from Bedrock\n",
    "summarisation_prompt = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=summarisation_prompt_id\n",
    "    )\n",
    "#parse the response and retrieve the elements we need for later.\n",
    "summarisation_prompt_dict = llm_utils.get_elts_from_prompt_get_response(summarisation_prompt)\n",
    "\n",
    "#retrieve prompt details from Bedrock\n",
    "extraction_prompt = bedrock_agent_client.get_prompt(\n",
    "        promptIdentifier=extraction_prompt_id\n",
    "    )\n",
    "#parse the response and retrieve the elements we need for later.\n",
    "extraction_prompt_dict = llm_utils.get_elts_from_prompt_get_response(extraction_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groundtruth data generation\n",
    "\n",
    "Ideally, your groundtruth data should be generated either by your live production system from end users or from manually crafted evals as you want those to be of high quality and reviewed/approved. \n",
    "\n",
    "An intermediary method could be to manually create a small sample of ground truth data and use LLMs to create variations of them.\n",
    "\n",
    "In that notebook, because we are focusing on the tools and process for our evaluation pipeline primarily, we are taking a shortcut and we use a LLM to generate our groundtruth data which is not recommended for real life use cases.\n",
    "\n",
    "To improve the groundtruth LLM generated data in comparison to a normal output from our prompt, we have implemented and added a reflection loop for an \"Evaluator/Inspector\" LLM to review the generated output of an \"Actor\".\n",
    "\n",
    "See the link below for more info on that pattern:\n",
    "https://www.promptingguide.ai/techniques/reflexion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection graph with LangGraph library\n",
    "\n",
    "If you are interested in the details of the implementation, uncomment the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ../src/utils/reflection_graph.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration used for our reflection evaluator\n",
    "\n",
    "As we're using the langGraph library, we're using a ChatBedrockConverse wrapper from the langchain_aws library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "evaluator_llm = ChatBedrockConverse(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    max_tokens = 4096,\n",
    "    temperature = 0,\n",
    "    top_p = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundtruth data generation for sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code should take 2min to execute. we're using the asyncio library to parallelise the calls converse apis. \n",
    "\n",
    "You can have a look at the reflection_graph.py while you wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "#This line is required to allow Jupyter Notebook to run asynchronous code correctly, as Jupyter Notebook has its own event loop running in the background\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def on_task_done(task, task_number, total_tasks):\n",
    "    print(f\"Task #{task_number} completed successfully\")\n",
    "\n",
    "\n",
    "#Util function that generates our dataset\n",
    "async def generate_dataset_for_evaluation(semaphore, prompt_dict, evaluator_llm, actor_llm, reflection_graph, transcript, combined_data):\n",
    "    async with semaphore:\n",
    "\n",
    "        #format prompt with transcript\n",
    "        prompt = prompt_dict[\"prompt_text\"].format(transcript=transcript[\"transcript\"])\n",
    "\n",
    "        #generate response from LLM\n",
    "        generated_answer= llm_utils.converse_api_call_no_tool(prompt, \n",
    "                                \"\", \n",
    "                                bedrock_runtime, \n",
    "                                conversation_history= [], \n",
    "                                prefill=\"\",\n",
    "                                model_id=prompt_dict[\"modelId\"], \n",
    "                                temperature=prompt_dict[\"temperature\"], \n",
    "                                top_p=prompt_dict[\"topP\"], \n",
    "                                max_tokens=prompt_dict[\"maxTokens\"],\n",
    "                                debug=False)\n",
    "        \n",
    "        #generate groundtruth\n",
    "        groundtruth = await reflection_graph.run_graph(prompt)\n",
    "\n",
    "        combined_data.append({\"question\": transcript, \"answer\": generated_answer, \"groundtruth\": groundtruth})\n",
    "\n",
    "async def run_generate_for_all_dataset(transcripts, prompt_dict, evaluator_llm):\n",
    "\n",
    "    #used to store results\n",
    "    output = []\n",
    "\n",
    "    #configuring a ChatBedrockConverse llm object to pass to our langgraph reflection graph\n",
    "    actor_llm = ChatBedrockConverse(\n",
    "        model_id=prompt_dict[\"modelId\"],\n",
    "        max_tokens = prompt_dict[\"maxTokens\"],\n",
    "        temperature = prompt_dict[\"temperature\"],\n",
    "        top_p = prompt_dict[\"topP\"]\n",
    "    )\n",
    "    #instantiate our reflection graph.\n",
    "    reflection_graph = ReflectionGraph(actor_llm, evaluator_llm)\n",
    "\n",
    "    #with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Get the current event loop\n",
    "        #loop = asyncio.get_running_loop()\n",
    "\n",
    "\n",
    "    # Create a semaphore with the specified concurrency limit\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    #create tasks\n",
    "    total_tasks = len(transcripts)\n",
    "    tasks = []\n",
    "\n",
    "    for i, transcript in enumerate(transcripts, start=1):\n",
    "        task = asyncio.create_task(\n",
    "            generate_dataset_for_evaluation(semaphore, prompt_dict, evaluator_llm, actor_llm, reflection_graph, transcript, output)\n",
    "        )\n",
    "        # Add a callback to the task\n",
    "        task.add_done_callback(lambda t, task_number=i, total=total_tasks: on_task_done(t, task_number, total))\n",
    "        tasks.append(task)\n",
    "\n",
    "    #waiting to complete.\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_gen_data = asyncio.run(run_generate_for_all_dataset(transcripts, sentiment_prompt_dict, evaluator_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the dataset to use it later\n",
    "def reshape_data(data):\n",
    "    reshaped_data = dict()\n",
    "    reshaped_data[\"question\"] = []\n",
    "    reshaped_data[\"answer\"] = []\n",
    "    reshaped_data[\"groundtruth\"] = []\n",
    "\n",
    "    for triplet in data:\n",
    "        reshaped_data[\"question\"].append(triplet[\"question\"])\n",
    "        reshaped_data[\"answer\"].append(triplet[\"answer\"])\n",
    "        reshaped_data[\"groundtruth\"].append(triplet[\"groundtruth\"])\n",
    "\n",
    "    return reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_combined_data = reshape_data(sentiment_gen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly check how many difference we have between groundtruth and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_counter = 0\n",
    "number_questions = len(sentiment_combined_data[\"question\"])\n",
    "for i in range(number_questions):\n",
    "    if (sentiment_combined_data[\"answer\"][i] != sentiment_combined_data[\"groundtruth\"][i]):\n",
    "        difference_counter +=1\n",
    "print(f\"we have {difference_counter}/{number_questions} differences between groundtruth and answers\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_utils.save_dict_to_json(sentiment_combined_data, \"../generated/groundtruth/sentiment_gt.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundtruth data generation for summarisation\n",
    "\n",
    "We run the same functions as for the sentiment data generation but with a different prompt. \n",
    "\n",
    "Running the below cell should take 8-9min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarisation_gen_data = asyncio.run(run_generate_for_all_dataset(transcripts, summarisation_prompt_dict, evaluator_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarisation_combined_data = reshape_data(summarisation_gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing few examples\n",
    "number_questions = len(summarisation_combined_data[\"question\"])\n",
    "for i in range(0,2):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"-----------------------\")\n",
    "    print(f\"LLM answer:\\n{summarisation_combined_data[\"answer\"][i]}\\n\")\n",
    "    print(f\"Groundtruth:\\n{summarisation_combined_data[\"groundtruth\"][i]}\\n\")\n",
    "    print(\"--------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_utils.save_dict_to_json(summarisation_combined_data, \"../generated/groundtruth/summary_gt.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundtruth data generation for Theme extraction\n",
    "\n",
    "The below cell should take 2-3min to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_gen_data = asyncio.run(run_generate_for_all_dataset(transcripts, extraction_prompt_dict, evaluator_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_combined_data = reshape_data(extraction_gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing few examples\n",
    "number_questions = len(extraction_combined_data[\"question\"])\n",
    "for i in range(0,3):\n",
    "    print(f\"Example {i}:\\n\")\n",
    "    print(f\"LLM answer:\\n{extraction_combined_data[\"answer\"][i]}\\n\")\n",
    "    print(f\"Groundtruth:\\n{extraction_combined_data[\"groundtruth\"][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_utils.save_dict_to_json(extraction_combined_data, \"../generated/groundtruth/extraction_gt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

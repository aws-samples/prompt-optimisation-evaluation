{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data generation\n",
    "\n",
    "We generate data to be used for a variety of use cases that we will use for our evaluation process.\n",
    "\n",
    "We choose to generate Call Centre transcripts from which we can do theme extraction, categorisation, summarisation, 3 very common use cases for generative AI. However, this is just an example and we can easily customise those prompts to generate Doctors' transcripts or legal documents if needed for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3==1.34.144 reportlab==4.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "#adding our utils library to sys path\n",
    "import sys\n",
    "sys.path.append(\"../src/utils/\")\n",
    "import llm_utils\n",
    "importlib.reload(llm_utils)\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock_runtime = session.client(service_name='bedrock-runtime')\n",
    "bedrock = session.client(service_name='bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcripts generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate calls topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 20\n",
    "\n",
    "topic_system_prompt = \"\"\"\n",
    "You are a synthetic data generator expert at generating topics of conversations given a specific context.\n",
    "\"\"\"\n",
    "\n",
    "topic_prompt = \"\"\" \n",
    "Your task is to generate topics of conversations between a customer and a call centre agent working for company BCD.\n",
    "\n",
    "Company BCD provides an online video on demand platform streaming movies, series and live sport.\n",
    "Viewers are subscribing to company BCD's platform for an annual or monthly fee with different level of access to content (premium, sport, paid per view).\n",
    "Customer can access the service via various applications: Web, IOS, Android, Smart TV, PS5 and XBOX.\n",
    "\n",
    "Generate <number>{number}</number> different topics of conversation using factual and concise language.\n",
    "\n",
    "Output the response in well formatted JSON format in <answer> tag as per the example below.\n",
    "\n",
    "<example>\n",
    "    {\n",
    "        'topics': [\n",
    "            'Topic 1',\n",
    "            'Topic 2',\n",
    "            'Topic 3',\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "user_input = topic_prompt.replace(\"number\", str(number_of_topics))\n",
    "\n",
    "topics_dict = llm_utils.converse_api_call_no_tool(user_input, \n",
    "                              topic_system_prompt, \n",
    "                              bedrock_runtime, \n",
    "                              conversation_history= [], \n",
    "                              prefill=\"<answer>{\", \n",
    "                              model_id=model_id, \n",
    "                              temperature=0, \n",
    "                              top_p=1, \n",
    "                              max_tokens=4096,\n",
    "                              json_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Transcripts based on topics\n",
    "\n",
    "Note: I had to put a reminder for closing curly brackets to get it consistent. see the  \"IMPORTANT: Do not forget the closing curly bracket \"}\" for each transcript as shown in the example below.\" line in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_system_prompt = \"\"\"\n",
    "You are a synthetic data generator expert at generating conversations given a specific context.\n",
    "\"\"\"\n",
    "\n",
    "transcript_prompt = \"\"\" \n",
    "Your task is to generate <number>{number}</number> transcripts between a customer and a call centre agent on the following topic <topic>{topic}</topic>.\n",
    "\n",
    "Make sure that the transcripts are anchored into the following context: \n",
    "<context>Company BCD provides an online video on demand platform streaming movies, series and live sport.\n",
    "Viewers are subscribing to company BCD's platform for an annual or monthly fee with different level of access to content (premium, sport, paid per view).\n",
    "Customer can access the service via various applications: Web, IOS, Android, Smart TV, PS5 and XBOX.</context>\n",
    "\n",
    "The customer might express some of the following emotions when calling the support call centre: Satisfaction, Frustration, Anger, Anxiety, Impatience, Confusion, Urgency, Relief, Gratitude, Disappointment, Resignation\n",
    "\n",
    "The call centre agent should always keep professional and helpul and use an empathetic tone where required even when face with animosity from the customer.\n",
    "\n",
    "The conversation should be at least <word count>{word_count}</word count> words.\n",
    "\n",
    "Output the response as a well formatted JSON in <answer> tag as per the example below. \n",
    "\n",
    "IMPORTANT: Do not forget the closing curly bracket \"}\" for each transcript as shown in the example below.\n",
    "\n",
    "<example>\n",
    "    <answer>\n",
    "        [\n",
    "            {\"topic\" : \"{topic}\", \"transcript\":\"Customer 1: text, Agent 1: text, Customer 1: text, Agent 1: text\"},\n",
    "            {\"topic\" : \"{topic}\", \"transcript\":\"Customer 2: text, Agent 2: text, Customer 2: text, Agent 2: text\"},\n",
    "            {\"topic\" : \"{topic}\", \"transcript\":\"Customer 3: text, Agent 3: text, Customer 3: text, Agent 3: text\"}\n",
    "        ]\n",
    "    </answer>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate transcripts concurrently to speed up the generation (should take 2min with 10 threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "word_count = 500\n",
    "number_of_transcripts = 5\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "transcripts = []\n",
    "\n",
    "def generate_item(topic, word_count, number, prompt, system_prompt, bedrock_runtime, model_id):\n",
    "\n",
    "    prompt_user_input = prompt.replace(\"{topic}\", topic).replace(\"{word_count}\", str(word_count)).replace(\"{number}\", str(number))\n",
    "\n",
    "    transcript = llm_utils.converse_api_call_no_tool(prompt_user_input,\n",
    "                                           system_prompt,\n",
    "                                           bedrock_runtime,\n",
    "                                           conversation_history=[],\n",
    "                                           prefill=\"<answer>\",\n",
    "                                           model_id=model_id,\n",
    "                                           temperature=0.6,\n",
    "                                           top_p=0.8,\n",
    "                                           max_tokens=4096,\n",
    "                                           json_check=True)\n",
    "    return transcript\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # we create a list of future object by submitting the execution of the function with the different topic.\n",
    "    futures = [executor.submit(generate_item, topic, word_count, number_of_transcripts, transcript_prompt, transcript_system_prompt, bedrock_runtime, model_id) for topic in topics_dict['topics']]\n",
    "    # we iterate over the future event as they complete and we retrieve the output for each \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        transcripts.append(result)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows transcript of the first topic\n",
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we export the output in a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export array to jsonlines file\n",
    "file_path = '../generated/transcripts/transcripts.jsonl'\n",
    "\n",
    "with open(file_path, 'w') as outfile:\n",
    "    for topic_entry in transcripts:\n",
    "        for entry in topic_entry:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate FAQ documents\n",
    "\n",
    "Those documents will be used as part of our RAG solution that will be a component of the solution that we need to optimise and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_system_prompt = \"\"\"\n",
    "You are a synthetic data generator expert at generating customer support FAQs.\n",
    "\"\"\"\n",
    "\n",
    "faq_prompt = \"\"\" \n",
    "Your task is to generate <number>{number_faq}</number> FAQs based on the following topic <topic>{topic}</topic>.\n",
    "\n",
    "Make sure that the faqs are anchored into the following context: \n",
    "<context>Company BCD provides an online video on demand platform streaming movies, series and live sport.\n",
    "Viewers are subscribing to company BCD's platform for an annual or monthly fee with different level of access to content (premium, sport, paid per view).\n",
    "Customer can access the service via various applications: Web, IOS, Android, Smart TV, PS5 and XBOX.</context>\n",
    "\n",
    "The FAQ should include a question from a customer and a detailed response of at least <number>{word_count}</number> words.\n",
    "\n",
    "Output the response in <answer> tag using the well formatted JSON format as shown in the example.\n",
    "\n",
    "IMPORTANT: Do not forget the closing curly bracket \"}\" for each transcript as shown in the example below.\n",
    "\n",
    "<example>\n",
    "  <answer>\n",
    "    [\n",
    "      {\"topic\":\"{topic}\", \"faq\":\"Q: question 1, R: response 1\"},\n",
    "      {\"topic\":\"{topic}\", \"faq\":\"Q: question 2, R: response 2\"},\n",
    "      {\"topic\":\"{topic}\", \"faq\":\"Q: question 3, R: response 3\"}\n",
    "    ]\n",
    "  </answer>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the FAQS (should take 2-3 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 300\n",
    "number_faq = 3\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "faqs = []\n",
    "\n",
    "#we use the same generate_item function\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # we create a list of future object by submitting the execution of the function with the different topic.\n",
    "    futures = [executor.submit(generate_item, topic, word_count, number_faq, faq_prompt, faq_system_prompt, bedrock_runtime, model_id) for topic in topics_dict['topics']]\n",
    "    # we iterate over the future event as they complete and we retrieve the output for each \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        faqs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print faqs for the first topic\n",
    "faqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the output in a jsonlines formatted file in case we need it for later in the workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export array to jsonlines file\n",
    "file_path = '../generated/faqs/faqs.jsonl'\n",
    "\n",
    "with open(file_path, 'w') as outfile:\n",
    "    for faq_entry in faqs:\n",
    "        for entry in faq_entry:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also export it as pdf to upload to the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "\n",
    "#setting style for topic heading\n",
    "styles = getSampleStyleSheet()\n",
    "topic_style = styles['Heading1']\n",
    "\n",
    "counter = 1\n",
    "for faq_entry in faqs:\n",
    "    for entry in faq_entry:\n",
    "\n",
    "        file_path = f'../generated/faqs/faq-doc-{counter}.pdf'\n",
    "        counter += 1\n",
    "\n",
    "        # Create a PDF document\n",
    "        doc = SimpleDocTemplate(file_path, pagesize=letter)\n",
    "        elements = []\n",
    "\n",
    "        # Add the topic\n",
    "        topic_paragraph = Paragraph(entry['topic'], topic_style)\n",
    "        elements.append(topic_paragraph)\n",
    "\n",
    "        # Add a blank line\n",
    "        elements.append(Paragraph('', styles['BodyText']))\n",
    "\n",
    "        # Add the FAQ\n",
    "        faq_style = styles['BodyText']\n",
    "        faq_paragraph = Paragraph(entry['faq'], faq_style)\n",
    "        elements.append(faq_paragraph)\n",
    "        # Build the PDF\n",
    "        doc.build(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 bucket to store FAQ docs and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "s3_client = boto3.client('s3')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create S3 bucket to store the KB data source\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'bedrock-kb-eval-{s3_suffix}' \n",
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except Exception as e:\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload pdf documents from local directory to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_pdf_dir = \"../generated/faqs\"\n",
    "\n",
    "target_dir_s3 = os.path.join(bucket_name, \"faqs\")\n",
    "\n",
    "# Iterate over all files in the local directory\n",
    "for filename in os.listdir(local_pdf_dir):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(local_pdf_dir, filename)\n",
    "\n",
    "        # Upload the file to S3 with the \"faq/\" prefix\n",
    "        try:\n",
    "            s3_client.upload_file(file_path, bucket_name, f\"faqs/{filename}\")\n",
    "            print(f\"Uploaded {filename} to {bucket_name}/faqs/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store target_dir_s3\n",
    "%store bucket_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
